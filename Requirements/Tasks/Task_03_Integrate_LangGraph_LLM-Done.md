# Task 3: Integrate Basic LangGraph & LLM

**Phase:** 2 - Core Conversational Logic (Engine)

**Goal:** Replace the placeholder logic in the Orchestration Engine's `/chat` endpoint with actual conversational capability by integrating LangGraph and a Large Language Model (LLM).

**Requirements:**

1.  **Framework Integration:** Integrate the `langgraph` library into the `orchestrator/main.py` FastAPI application.
2.  **LLM Client:** Integrate an LLM client library (e.g., `langchain-openai`, `langchain-google-genai`, etc.) to interact with a chosen chat LLM.
3.  **State Definition:** Define a basic state structure within LangGraph to manage the conversation history (e.g., a list of messages).
4.  **Graph Definition:** Create a simple LangGraph graph with at least one node responsible for:
    *   Taking the current conversation state (history).
    *   Calling the configured chat LLM with the history.
    *   Updating the state with the LLM's response.
5.  **Endpoint Modification:** Modify the `/chat` POST endpoint in `orchestrator/main.py`:
    *   Maintain the conversation state across requests (initially, this could be simple in-memory state per request or a very basic global state; more robust state management can be added later).
    *   Append the incoming user message to the state's history.
    *   Invoke the LangGraph graph with the current state.
    *   Extract the latest AI response from the graph's resulting state.
    *   Return this AI response in the JSON payload (e.g., `{"response": "actual LLM response"}`).
6.  **Configuration:** Ensure LLM API keys and any other necessary configurations (e.g., model name) are handled securely, preferably via environment variables (`dotenv` library can be used).

**Testable Outcome:**

*   With the Orchestration Engine running and the UI (from Task 2) connected:
    *   Sending a message through the UI results in a relevant, conversational response generated by the configured LLM being displayed in the UI, replacing the placeholder response from Task 1.
    *   Subsequent messages sent within a short period (testing the basic state mechanism) should show the LLM having some context of the previous turn(s).
*   The Orchestration Engine logs should indicate successful interaction with the LLM API.
*   Invalid API keys or configuration should result in appropriate errors logged by the engine.